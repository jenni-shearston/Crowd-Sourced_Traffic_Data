# Tutorial: Create Google Traffic Timeseries: Spatial Polygons
# Project: Acquisition and Analysis of Crowd-Sourced Traffic Data at Varying Spatial Scales
# Script Authors: Jenni A. Shearston and Sebastian T. Rowland
# Updated: 03/02/2022

####***********************
#### Table of Contents #### 
####***********************

# N: Notes
# 0: Preparation 
# 1: Prepare Shapefile of Interest
# 2: Prepare Polygon and Raster Inputs
# 3: Prepare Vectors of captured_datetimes of Interest
# 4: Create Google Traffic timeseries: Polygons

####**************
#### N: Notes ####
####**************

# Na Description
# In this script we present a tutorial for creating a time series from processed
# Google Traffic images, for spatial polygons. As an example, we include one week of 
# processed data for a subset of the NYC area (every 3-hrs, n=56 images), and create a time
# series aggregated to South Bronx census tracts. Users can edit the parameters described
# below to complete their own analysis with differing traffic map areas, time periods,
# and spatial polygons. We strongly recommend you clone this repository to ensure you 
# have the same file structure and subfolders as used in the script and functions
# used below. For definitions of variables created in timeseries output, see glossary.

# Nb For your own analysis
# Here we list everything you will need to change to run your own version of
# this script.
# You will need to add your own files to the following directories:
#     data/polygons_of_interest: add the shapefile you will aggregate Google 
#       Traffic colors within and specify path details (line 97)
#     data/gt_refs: add a geotiff from the Google Traffic map area you have data 
#       for that has been georeferenced and projected in WGS84 (gt_geo_projected)
#       and specify the name of that file (line 127)
#     data/gt_image_cat: add a processed Google Traffic image from your traffic
#       map area (captured_datetime is arbitrary) and specify file name (line 139)
# You will need to assign the following directories: 
#     gt_dir: the file path where processed Google Traffic images are stored (line 198)
#     dir_output: the file path where you would like the .fst file containing 
#       the aggregated time series to be saved. (line 224)
#     For directories, note that the root of the directory is specified
#     to be the R project package (confirm with 'here()'), so include the file 
#     path from the R project location rather than the full file path
# You will need to specify the following variables/names:
#     poly_id_var: the variable that uniquely identifies each polygon in your shapefile (line 103)
#     base_date: the base or start datetime for your analysis (line 191)
#     end_date: the end datetime for your analysis (line 192)
#     name_output: the name you would like to give the .fst file with the aggregated 
#       timeseries (line 225)

####********************
#### 0: Preparation #### 
####********************

# 0a Specify needed packages
packages <- c('tidyverse', 'raster', 'rgdal', 'terra', 'sf', 'here', 'doParallel',
              'tictoc', 'png', 'fst', 'lubridate', 'stringr', 'parallel', 'foreach')

# 0b Load or install and load all packages
lapply(packages, FUN = function(x){
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)}
  })
rm(packages)

# 0c Source our functions
# 0c.i Get the names of all of the scripts that are just functions
myFunctions <- list.files(path = here::here('Rfunctions'))

# 0c.ii Define function to run sources 
source_myFunction <- function(FunctionName){
  source(here::here('Rfunctions', FunctionName))
}

# 0c.iii Source all the function scripts
#        Note: We don't actually need the assignment, it just 
#              removes annoying output generated by the sourcing code. 
#              Since we are just sourcing these, we can use map. 
a <- purrr::map(myFunctions, source_myFunction)
rm(a, myFunctions)

####**************************************
#### 1: Prepare Shapefile of Interest #### 
####**************************************

# 1a Bring in shapefile of interest
#    Note: Place your shapefile in the 'polygons_of_interest' subfolder 
#          within the 'data' folder and change 'bronx_census_tracts' and 
#          'bronx.shp' to reflect your file path
polygons_of_interest <- st_read(here::here('data', 'polygons_of_interest', 
                                           'bronx_census_tracts', 'bronx.shp'))

# 1b Specify poly_id_var
#    Note: This is the variable that uniquely identifies each polygon
#          Replace 'geoid' with the name of the unique polygon id
#          in your shapefile
poly_id_var <- 'geoid'

####*******************************************
#### 2: Prepare Polygon and Raster Inputs #### 
####*******************************************

# The code in sections 2 and 3 needs to be run before the get_gt_agg_timeseries  
# function that loops through every Google Traffic image (gt_image_cat) 
# to put all key variables and datasets in the Global Environment 

# 2a Rename the poly_id and remove non-essential variables 
polygons_of_interest <- polygons_of_interest %>% 
  dplyr::rename(poly_id = !!poly_id_var) %>% 
  dplyr::select(poly_id)

# 2b Make poly_id numeric
polygons_of_interest <- polygons_of_interest %>% 
  dplyr::mutate(poly_id = as.numeric(poly_id))

# 2c Read in gt_geo_projected
#    Note: Replace 'gt_geo_projected' with a Geotiff from your Google 
#          Traffic map area that has been georeferenced and projected 
#          using WGS84
gt_geo_projected <- raster::raster(here::here('data', 'gt_refs',
                                              'gt_geo_projected.tif'))

# 2d Extract the extent (min and max lat and long) of gt_geo_projected 
#    Note: We use gt_geo_projected's extent because we have carefully 
#          georeferenced it
gt_extent <- raster::extent(gt_geo_projected)

# 2e Read a gt_image_cat.png as a matrix
#    Note: The actual datetime of this image is arbitrary. Replace 
#          'CCC_01_01_18__02_00.png' with a processed and categorized
#          image from your own Google Traffic map area
gt_matrix_cat <- png::readPNG(here::here('data', 'gt_image_cat', 
                                         'CCC_01_01_18__02_00.png'))

# 2f Convert gt_matrix_unprojected to raster
gt_raster_unprojected <- raster::raster(gt_matrix_cat) 

# 2g Change the extent of gt_raster_unprojected to reflect the extent of the gt_extent 
#    Note: Now we are converting it to lat long 
#          Here we georeference gt_raster_unprojected based on the location of 
#          gt_geo_projected which was georeferenced carefully
raster::extent(gt_raster_unprojected) <- c(gt_extent[1], gt_extent[2], gt_extent[3], gt_extent[4])

# 2h Convert polygons_of_interest CRS to WGS84 
#    Note: This is important because gt_raster_projected has a CRS of WGS84 which 
#          uses lat/long and not decimal degrees or feet 
#          Because gt_raster_unprojected has been assigned the extent of 
#          gt_raster_projected, which is in lat/long, the polygons_of_interest
#          file must have a CRS in lat/long in order for the rasterize
#          command to clip the shapefile to the gt_raster_unprojected's extent
polygons_of_interest_wgs84 <- sf::st_transform(polygons_of_interest, "WGS84")

# 2i Convert polygons_of_interest to a raster with the dimensions and 
#    resolution of gt_raster_unprojected (~ 30s to 2min)
#    Note: The resulting raster will only have cells within the area defined by the 
#          overlap of gt_raster_unprojected and the polygons of interest 
#          The raster will have one column - the id of the polygon the cell belongs to
poly_gt_crosswalk <- raster::rasterize(polygons_of_interest_wgs84, 
                                       gt_raster_unprojected, field = "poly_id")

# 2j Convert to matrix
poly_matrix <- raster::as.matrix(poly_gt_crosswalk)

# 2k Remove no longer needed files and run garbage collection to 
#    return memory 
#    Note: This is critical to conserve memory
rm(poly_gt_crosswalk, polygons_of_interest_wgs84, 
   gt_raster_unprojected, gt_extent, gt_geo_projected,
   gt_matrix_cat)
gc()

####**********************************************************
#### 3: Prepare vectors of captured_datetimes of interest #### 
####**********************************************************

# 3a Create vector of captured_datetimes of interest
#    Note: Here we use January 10, 2020 through January 16, 2020
#          Processed images for a subset of the NYC area are included in
#          the data folder for this time period, at 3-hour intervals (n = 56).
#          There is no need to specify only the specific 3-hour intervals in your
#          captured datetime vector; the function will fill an NA for any datetimes
#          between the base_date and end_date that gt_image_cats are not present for
#          Change the base_date and end_date to reflect your datetimes of interest.
captured_datetime_vector <- make_captured_datetime_vector(
  base_date = '2020/01/10 00:30',
  end_date = '2020/01/16 21:30')

# 3b Set directory where processed Google Traffic images are stored
#    Note: Change this directory to lead to where your processed Google Traffic
#          images are stored. Note that the root directory should be this R package
#          Check with 'here()' command to confirm
gt_dir <- here::here('data', 'gt_image_cat', 'bronx_example')

# 3c Reformat vector of captured_datetimes of interest
captured_datetime_vector_formatted <- reformat_captured_datetime_vector(
  captured_datetime_vector = captured_datetime_vector, 
  gt_dir = gt_dir)


####***************************************************
#### 4: Create Google Traffic timeseries: polygons #### 
####***************************************************

# 4a Run function to aggregate Google Traffic images to polygons
#    Note: Set the file path where you would like the .fst file containing 
#          the time series to be saved by assigning dir_output. Note that the 
#          root directory should be this R package - check with 'here()' command to confirm
#          Change the name you would like the .fst file to have with name_output
#          It is highly recommended that you first run the function for one 
#          week of time, to get a sense of how long it will take to run
#          your full datetime vector. A year's worth of analysis for a large
#          city at hourly resolution may take ~ 10-15 hours.
#          method can be set to 'parallel' to use parallelization (n-1 cores) 
#          or 'forloop' to use a single core
tictoc::tic('completes 1 week of bronx tracts')
gt_timeseries <- 
  get_gt_agg_timeseries(captured_datetime_vector_filename = captured_datetime_vector_formatted, 
                        dir_output = 'outputs/Rtutorials', 
                        name_output = 'bronx_polygons_example_timeseries',
                        gt_dir = gt_dir,
                        method = 'parallel')
tictoc::toc()



